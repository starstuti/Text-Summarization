{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Text Summarization System.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJC_chbssB9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b7582f-7ae7-4cf4-ae86-dd6be72420b0"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import csv\n",
        "from nltk.tag import pos_tag # for proper noun\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import pandas as pd\n",
        "import math\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "ps = PorterStemmer()\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuGkNR-JsB9N"
      },
      "source": [
        "# ......................part 1 (cue phrases).................\n",
        "def cue_phrases():\n",
        "    QPhrases=[\"incidentally\", \"example\", \"anyway\", \"furthermore\",\"according\",\n",
        "            \"first\", \"second\", \"then\", \"now\", \"thus\", \"moreover\", \"therefore\", \"hence\", \"lastly\", \"finally\", \"summary\"]\n",
        "\n",
        "    cue_phrases={}\n",
        "    for sentence in sent_tokens:\n",
        "        cue_phrases[sentence] = 0\n",
        "        word_tokens = nltk.word_tokenize(sentence)\n",
        "        for word in word_tokens:\n",
        "            if word.lower() in QPhrases:\n",
        "                cue_phrases[sentence] += 1\n",
        "    maximum_frequency = max(cue_phrases.values())\n",
        "    for k in cue_phrases.keys():\n",
        "        try:\n",
        "            cue_phrases[k] = cue_phrases[k] / maximum_frequency\n",
        "            cue_phrases[k]=round(cue_phrases[k],3)\n",
        "        except ZeroDivisionError:\n",
        "            x=0\n",
        "    print(cue_phrases.values())\n",
        "    return cue_phrases"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_xiqMxxsB9P"
      },
      "source": [
        "# .......................part2 (numerical data)...................\n",
        "def numeric_data():\n",
        "    numeric_data={}\n",
        "    for sentence in sent_tokens:\n",
        "        numeric_data[sentence] = 0\n",
        "        word_tokens = nltk.word_tokenize(sentence)\n",
        "        for k in word_tokens:\n",
        "            if k.isdigit():\n",
        "                numeric_data[sentence] += 1\n",
        "    maximum_frequency = max(numeric_data.values())\n",
        "    for k in numeric_data.keys():\n",
        "        try:\n",
        "            numeric_data[k] = (numeric_data[k]/maximum_frequency)\n",
        "            numeric_data[k] = round(numeric_data[k], 3)\n",
        "        except ZeroDivisionError:\n",
        "            x=0\n",
        "    print(numeric_data.values())\n",
        "    return numeric_data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f39szesfsB9Q"
      },
      "source": [
        "#....................part3(sentence length)........................\n",
        "def sent_len_score():\n",
        "    sent_len_score={}\n",
        "    for sentence in sent_tokens:\n",
        "        sent_len_score[sentence] = 0\n",
        "        word_tokens = nltk.word_tokenize(sentence)\n",
        "        length=len(word_tokens)\n",
        "        if length in range(0,10):\n",
        "            sent_len_score[sentence]=1-0.05*(10-length)\n",
        "        elif len(word_tokens) in range(10,20):\n",
        "            sent_len_score[sentence]=1\n",
        "        else:\n",
        "            sent_len_score[sentence]=1-(0.05)*(length-20)\n",
        "    for k in sent_len_score.keys():\n",
        "        sent_len_score[k]=round(sent_len_score[k],4)\n",
        "    print(sent_len_score.values())\n",
        "    return sent_len_score"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8TxaVd-sB9S"
      },
      "source": [
        "#....................part4(sentence position)........................\n",
        "def sentence_position():\n",
        "    sentence_position={}\n",
        "    d=1\n",
        "    no_of_sent=len(sent_tokens)\n",
        "    for i in range(no_of_sent):\n",
        "        a=1/d\n",
        "        b=1/(no_of_sent-d+1)\n",
        "        sentence_position[sent_tokens[d-1]]=max(a,b)\n",
        "        d=d+1\n",
        "    for k in sentence_position.keys():\n",
        "        sentence_position[k]=round(sentence_position[k],3)\n",
        "    print(sentence_position.values())\n",
        "    return sentence_position"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huWNByyFsB9S"
      },
      "source": [
        "#Create a frequency table to compute the frequency of each word.\n",
        "def word_frequency():\n",
        "    freqTable = {}\n",
        "    for word in word_tokens_refined:    \n",
        "        if word in freqTable:         \n",
        "            freqTable[word] += 1    \n",
        "        else:         \n",
        "            freqTable[word] = 1\n",
        "    for k in freqTable.keys():\n",
        "        freqTable[k]= math.log10(1+freqTable[k])\n",
        "#Compute word frequnecy score of each sentence\n",
        "    word_frequency={}\n",
        "    for sentence in sent_tokens:\n",
        "        word_frequency[sentence]=0\n",
        "        e=nltk.word_tokenize(sentence)\n",
        "        f=[]\n",
        "        for word in e:\n",
        "            f.append(ps.stem(word))\n",
        "        for word,freq in freqTable.items():\n",
        "            if word in f:\n",
        "                word_frequency[sentence]+=freq\n",
        "    maximum=max(word_frequency.values())\n",
        "    for key in word_frequency.keys():\n",
        "        try:\n",
        "            word_frequency[key]=word_frequency[key]/maximum\n",
        "            word_frequency[key]=round(word_frequency[key],3)\n",
        "        except ZeroDivisionError:\n",
        "            x=0\n",
        "    print(word_frequency.values())\n",
        "    return word_frequency"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dncr5cPCsB9U"
      },
      "source": [
        "#........................part 6 (upper cases).................................\n",
        "def upper_case():\n",
        "    upper_case={}\n",
        "    for sentence in sent_tokens:\n",
        "        upper_case[sentence] = 0\n",
        "        word_tokens = nltk.word_tokenize(sentence)\n",
        "        for k in word_tokens:\n",
        "            if k.isupper():\n",
        "                upper_case[sentence] += 1\n",
        "    maximum_frequency = max(upper_case.values())\n",
        "    for k in upper_case.keys():\n",
        "        try:\n",
        "            upper_case[k] = (upper_case[k]/maximum_frequency)\n",
        "            upper_case[k] = round(upper_case[k], 3)\n",
        "        except ZeroDivisionError:\n",
        "            x=0\n",
        "    print(upper_case.values())\n",
        "    return upper_case"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INjXW_iJsB9W"
      },
      "source": [
        "#......................... part7 (number of proper noun)...................\n",
        "def proper_noun():\n",
        "    proper_noun={}\n",
        "    for sentence in sent_tokens:\n",
        "        tagged_sent = pos_tag(sentence.split())\n",
        "        propernouns = [word for word, pos in tagged_sent if pos == 'NNP']\n",
        "        proper_noun[sentence]=len(propernouns)\n",
        "    maximum_frequency = max(proper_noun.values())\n",
        "    for k in proper_noun.keys():\n",
        "        try:\n",
        "            proper_noun[k] = (proper_noun[k]/maximum_frequency)\n",
        "            proper_noun[k] = round(proper_noun[k], 3)\n",
        "        except ZeroDivisionError:\n",
        "            x=0\n",
        "    print(proper_noun.values())\n",
        "    return proper_noun"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s5E2eF3sB9Z"
      },
      "source": [
        "#................................. part 8 (word matches with heading) ...................\n",
        "def head_match():\n",
        "    head_match={}\n",
        "    heading=sent_tokens[0]\n",
        "    for sentence in sent_tokens:\n",
        "        head_match[sentence]=0\n",
        "        word_tokens = nltk.word_tokenize(sentence)\n",
        "        for k in word_tokens:\n",
        "            if k not in stopWords:\n",
        "                k = ps.stem(k)\n",
        "                if k in ps.stem(heading):\n",
        "                    head_match[sentence] += 1\n",
        "    maximum_frequency = max(head_match.values())\n",
        "    for k in head_match.keys():\n",
        "        try:\n",
        "            head_match[k] = (head_match[k]/maximum_frequency)\n",
        "            head_match[k] = round(head_match[k], 3)\n",
        "        except ZeroDivisionError:\n",
        "            x=0\n",
        "    print(head_match.values())\n",
        "    return head_match"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTKIEm0IsB9b"
      },
      "source": [
        "import math\n",
        "\n",
        "def create_frequency_matrix(sentences):\n",
        "    frequency_matrix = {}\n",
        "\n",
        "    for sent in sentences:\n",
        "        freq_table = {}\n",
        "        words = word_tokenize(sent)\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            word = ps.stem(word)\n",
        "            if word in stopWords:\n",
        "                pass\n",
        "            if word in freq_table:\n",
        "                freq_table[word] += 1\n",
        "            else:\n",
        "                freq_table[word] = 1\n",
        "\n",
        "        frequency_matrix[sent] = freq_table\n",
        "\n",
        "    return frequency_matrix\n",
        "\n",
        "\n",
        "def create_tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        tf_table = {}\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, count in f_table.items():\n",
        "            tf_table[word] = count / count_words_in_sentence\n",
        "\n",
        "        tf_matrix[sent] = tf_table\n",
        "\n",
        "    return tf_matrix\n",
        "\n",
        "\n",
        "def create_documents_per_words(freq_matrix):\n",
        "    word_per_doc_table = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "          # occureance of word in sentences\n",
        "            if word in word_per_doc_table:\n",
        "                word_per_doc_table[word] += 1\n",
        "            else:\n",
        "                word_per_doc_table[word] = 1\n",
        "\n",
        "    return word_per_doc_table\n",
        "\n",
        "\n",
        "def create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
        "  # total_documents=total_sentences\n",
        "  # inverse document frequency\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix\n",
        "\n",
        "\n",
        "def create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "\n",
        "        tf_idf_table = {}\n",
        "\n",
        "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
        "                                                    f_table2.items()):  \n",
        "            tf_idf_table[word1] = float(value1 * value2)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix\n",
        "\n",
        "\n",
        "def score_sent(tf_idf_matrix):\n",
        "\n",
        "    sentenceValue = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_score_per_sentence = 0\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, score in f_table.items():\n",
        "            total_score_per_sentence += 0.5*score\n",
        "\n",
        "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
        "\n",
        "    return sentenceValue\n",
        "\n",
        "\n",
        "def tfidf():\n",
        "    sentences = sent_tokens\n",
        "    total_documents = len(sentences)\n",
        "\n",
        "    freq_matrix = create_frequency_matrix(sentences)\n",
        "    #print(freq_matrix)\n",
        "\n",
        "    tf_matrix = create_tf_matrix(freq_matrix)\n",
        "    #print(tf_matrix)\n",
        "\n",
        "    count_doc_per_words = create_documents_per_words(freq_matrix)\n",
        "    #print(count_doc_per_words)\n",
        "\n",
        "    idf_matrix = create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
        "    \n",
        "    tf_idf_matrix = create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "\n",
        "    tfidf_score = score_sent(tf_idf_matrix)\n",
        "\n",
        "    print(tfidf_score.values())\n",
        "    return tfidf_score\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6X1VIZ6RsB9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91fe3cc7-d6ef-4e68-a174-9793beac9309"
      },
      "source": [
        "df=pd.DataFrame(columns=['a','b','c','d','upper','f','g','h','tfidf','key','label'])\n",
        "print(df)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [a, b, c, d, upper, f, g, h, tfidf, key, label]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp12L1UQsB9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "09483433-dd0e-4739-e0da-74f420e491cc"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "path1='C:\\\\Users\\\\Priyanka\\\\Desktop\\\\COVID_19_dataset\\\\documents\\\\'\n",
        "path2='C:\\\\Users\\\\Priyanka\\\\Desktop\\\\COVID_19_dataset\\\\summary\\\\'\n",
        "\n",
        "filelist = os.listdir(path1)\n",
        "for file in filelist:\n",
        "    f = open(path1+file, \"r\")\n",
        "    f1=open(path2+file,\"r\")\n",
        "    text=f.read()\n",
        "    text1=f1.read()\n",
        "    # documents open\n",
        "    sent_tokens = nltk.sent_tokenize(text)\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    # summary open\n",
        "    sent_tokens1 = nltk.sent_tokenize(text1)\n",
        "    word_tokens1 = nltk.word_tokenize(text1)\n",
        "    word_tokens_lower=[word.lower() for word in word_tokens]\n",
        "    stopWords = list(set(stopwords.words(\"english\")))\n",
        "    word_tokens_refined=[x for x in word_tokens_lower if x not in stopWords]\n",
        "    g=cue_phrases()\n",
        "    z=list(g.keys())\n",
        "    g=list(g.values())\n",
        "    h=numeric_data()\n",
        "    h=list(h.values())\n",
        "    i=sent_len_score()\n",
        "    i=list(i.values())\n",
        "    j=sentence_position()\n",
        "    j=list(j.values())   \n",
        "    p=upper_case()\n",
        "    p=list(p.values())\n",
        "    l=head_match()\n",
        "    l=list(l.values())\n",
        "    m=word_frequency()\n",
        "    m=list(m.values())\n",
        "    n=proper_noun()\n",
        "    n=list(n.values())\n",
        "    p=tfidf()\n",
        "    p=list(p.values())\n",
        "    label={}\n",
        "    for sentence in sent_tokens:\n",
        "        label[sentence]=0\n",
        "        for sentence1 in sent_tokens1:\n",
        "            if(sentence==sentence1):\n",
        "                label[sentence]+=1\n",
        "                \n",
        "    o=list(label.values())\n",
        "    df = df.append(pd.DataFrame({'a': g,'b': h,'c': i,'d': j,'upper': p,'f': l,'g': m,'h': n,'tfidf':p,'key': z,'label': o}), ignore_index=True)\n",
        "    \n",
        "    f.close()\n",
        "    f1.close()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ce4524e35ecd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C:\\\\Users\\\\Priyanka\\\\Desktop\\\\COVID_19_dataset\\\\summary\\\\'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfilelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilelist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Priyanka\\\\Desktop\\\\COVID_19_dataset\\\\documents\\\\'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnrK6I8zsB9g"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIgofbSdsB9h"
      },
      "source": [
        "df=df.to_csv('C:\\\\Users\\\\Priyanka\\\\Desktop\\\\COVID_19_dataset\\\\output.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPM4WdZwsB9h"
      },
      "source": [
        "df=pd.read_csv('C:\\\\Users\\\\Priyanka\\\\Desktop\\\\COVID_19_dataset\\\\output.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh8Wmd1FsB9i"
      },
      "source": [
        "training=df[df.columns[0:8]]\n",
        "test=df[df.columns[-1]]\n",
        "# print(test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ4apdkQsB9i"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "X_train, X_test, y_train, y_test = train_test_split(training, test, test_size=0.3,random_state=109)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-p5LAETsB9i"
      },
      "source": [
        "def logistic():   \n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    clf2 = LogisticRegression(random_state=0)\n",
        "    #Train the model using the training sets\n",
        "    clf2.fit(X_train, y_train)\n",
        "\n",
        "    #Predict the response for test dataset\n",
        "    y_pred = clf2.predict(X_test) \n",
        "    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8cr1bFbsB9j"
      },
      "source": [
        "logistic()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIQ0Tr-BsB9j"
      },
      "source": [
        "# voting ensemble\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "model1 = LogisticRegression(random_state=1)\n",
        "model2 = DecisionTreeClassifier(random_state=2,max_depth=5)\n",
        "model3 = KNeighborsClassifier()\n",
        "# model4 = SVC(random_state=3,kernel='linear')\n",
        "model = VotingClassifier(estimators=[('lr', model1), ('dt', model2),('knn',model3)], voting='hard')\n",
        "model.fit(X_train,y_train)\n",
        "model.score(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksmM_ej2sB9k"
      },
      "source": [
        "# stacking ensemble\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "def Stacking(model,train,y,test,n_fold):\n",
        "   folds=StratifiedKFold(n_splits=n_fold,random_state=1)\n",
        "   test_pred=[0]*test.shape[0]\n",
        "   train_pred=np.empty((0,1),float)\n",
        "   for train_indices,val_indices in folds.split(train,y.values):\n",
        "      res=[]\n",
        "      x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]\n",
        "      y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]\n",
        "\n",
        "      model.fit(X=x_train,y=y_train)\n",
        "      train_pred=np.append(train_pred,model.predict(x_val))\n",
        "      pred=model.predict(test)\n",
        "      for i in range(len(pred)):\n",
        "            res.append(pred[i]+test_pred[i])\n",
        "      test_pred=res\n",
        "   test_pred=[each/10 for each in test_pred]\n",
        "   return test_pred,train_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXCZorHQsB9k"
      },
      "source": [
        "model1 = DecisionTreeClassifier(random_state=1, max_depth=4)\n",
        "test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=X_train,test=X_test,y=y_train)\n",
        "train_pred1=pd.DataFrame(train_pred1)\n",
        "test_pred1=pd.DataFrame(test_pred1)\n",
        "model2 = SVC(random_state=3,kernel='linear')\n",
        "test_pred2 ,train_pred2=Stacking(model=model2,n_fold=10,train=X_train,test=X_test,y=y_train)\n",
        "train_pred2=pd.DataFrame(train_pred2)\n",
        "test_pred2=pd.DataFrame(test_pred2)\n",
        "\n",
        "df = pd.concat([train_pred1, train_pred2], axis=1)\n",
        "df_test = pd.concat([test_pred1, test_pred2], axis=1)\n",
        "# print(df_test)\n",
        "model = LogisticRegression(random_state=1)\n",
        "model.fit(df,y_train)\n",
        "# print(X_test.shape)\n",
        "model.score(df_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tm7CBFAsB9l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv3xvvjOsB9l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}